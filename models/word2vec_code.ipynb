{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sJFD4fM4c1q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.spatial.distance import cosine\n",
        "from gensim.models.word2vec import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## W2V"
      ],
      "metadata": {
        "id": "QHDq8F05LVkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_freqs(words):\n",
        "    \"\"\" Returns dictionary frequency of word frequencies of words in a list. \"\"\"\n",
        "    freqs = {w: 0 for w in words}\n",
        "    for word in words:\n",
        "        freqs[word] += 1\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "rmOprJmS6rBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vector_dict(vec_fp, words):\n",
        "    \"\"\" Utility to load word embeddings from a .vec file into a dictionary. \"\"\"\n",
        "    vdict = {}\n",
        "\n",
        "    model = Word2Vec.load(vec_fp)\n",
        "\n",
        "    for word in words:\n",
        "        try:\n",
        "            vdict[word] = model.wv[word]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    #assert len(vdict) == len(words), \"Not all target words were found in the *.vec file {}!\".format(vec_fp)\n",
        "\n",
        "    return vdict"
      ],
      "metadata": {
        "id": "2ZCOfXnV67XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, export_fp, targets, filtered=True):\n",
        "    \"\"\" Preprocesses a single text by filtering words for frequency and sentences for length. \"\"\"\n",
        "\n",
        "    lines = text.split(\"\\n\")\n",
        "    all_words = text.split()\n",
        "\n",
        "    if filtered:\n",
        "\n",
        "        # determine threshold\n",
        "        min_freq = len(lines) // 50_000\n",
        "\n",
        "        word_freqs = get_word_freqs(all_words)\n",
        "\n",
        "        keep_words = set([word for word, freq in word_freqs.items() if freq >= min_freq])\n",
        "        target_words = set(targets)\n",
        "\n",
        "        # print([x for x in word_freqs if x.startswith('dem')])\n",
        "\n",
        "        missing_targets = target_words - keep_words\n",
        "\n",
        "        # for target in targets:\n",
        "        #     # Print freq\n",
        "        #     print('Freq:',target, word_freqs[target])\n",
        "\n",
        "        if len(missing_targets) > 0:\n",
        "            print(\"Keeping the following target words despite frequencies below {}:\\n\".format(min_freq), missing_targets)\n",
        "            keep_words = keep_words.union(target_words)\n",
        "\n",
        "    else:\n",
        "\n",
        "        keep_words = set(all_words)\n",
        "\n",
        "\n",
        "    new_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        new_words = [word for word in line.split() if word in keep_words]\n",
        "        if len(new_words) > 1:\n",
        "            new_lines.append(\" \".join(new_words))\n",
        "\n",
        "    with open(export_fp, \"w+\") as fh:\n",
        "        fh.write(\"\\n\".join(new_lines))"
      ],
      "metadata": {
        "id": "pJFTcx479FbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_texts(c1_path, c2_path, c3_path, targets_path, experiment_dir):\n",
        "    \"\"\" Removes words below the frequency threshold in both corpora. \"\"\"\n",
        "\n",
        "    with open(c1_path, \"r\") as fh:\n",
        "        c1 = fh.read()\n",
        "\n",
        "    with open(c2_path, \"r\") as fh:\n",
        "        c2 = fh.read()\n",
        "\n",
        "    with open(c3_path, \"r\") as fh:\n",
        "        c3 = fh.read()\n",
        "\n",
        "    with open(targets_path, 'r', encoding='utf-8') as f:\n",
        "        targets = [word.strip() for word in f.readlines()]\n",
        "\n",
        "    prep_dir = experiment_dir + \"preprocessed_texts/\"\n",
        "    os.makedirs(prep_dir, exist_ok=True)\n",
        "\n",
        "    preprocess_text(c1, prep_dir + \"c1.txt\", targets)\n",
        "    preprocess_text(c2, prep_dir + \"c2.txt\", targets)\n",
        "    preprocess_text(c3, prep_dir + \"c3.txt\", targets)"
      ],
      "metadata": {
        "id": "mx1N_kgu8dwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(experiment_dir, n_window=10, dim=300, **kwargs):\n",
        "    \"\"\" Vectorizes all words in the two corpora separately with Word2Vec. \"\"\"\n",
        "\n",
        "    vec_dir = experiment_dir + \"word_representations/\"\n",
        "    os.makedirs(vec_dir, exist_ok=True)\n",
        "\n",
        "    prep_dir = experiment_dir +  \"preprocessed_texts/\"\n",
        "\n",
        "    with open(prep_dir + \"c1.txt\", 'r', encoding='utf-8') as f1:\n",
        "        sentences1 = [[word for word in sent.split()] for sent in f1.readlines()]\n",
        "\n",
        "    with open(prep_dir + \"c2.txt\", 'r', encoding='utf-8') as f2:\n",
        "        sentences2 = [[word for word in sent.split()] for sent in f2.readlines()]\n",
        "\n",
        "    with open(prep_dir + \"c3.txt\", 'r', encoding='utf-8') as f3:\n",
        "        sentences3 = [[word for word in sent.split()] for sent in f3.readlines()]\n",
        "\n",
        "    model1 = Word2Vec(sentences1, vector_size=dim, window=n_window, min_count=1)\n",
        "    model1.save(vec_dir + \"c1.vec\")\n",
        "\n",
        "    model2 = Word2Vec(sentences2, vector_size=dim, window=n_window, min_count=1)\n",
        "    model2.save(vec_dir + \"c2.vec\")\n",
        "\n",
        "    model3 = Word2Vec(sentences3, vector_size=dim, window=n_window, min_count=1)\n",
        "    model3.save(vec_dir + \"c3.vec\")"
      ],
      "metadata": {
        "id": "gVMJeiZJ9MIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_align_gensim(m1, m2):\n",
        "    \"\"\"\n",
        "     Intersect two gensim word2vec models.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the vocab for each model\n",
        "    vocab_m1 = set(m1.wv.index_to_key)\n",
        "    vocab_m2 = set(m2.wv.index_to_key)\n",
        "\n",
        "    # Find the common vocabulary\n",
        "    common_vocab = vocab_m1 & vocab_m2\n",
        "\n",
        "    # If no alignment necessary because vocab is identical...\n",
        "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
        "        return (m1, m2)\n",
        "\n",
        "    # Otherwise sort by frequency (summed for both)\n",
        "    common_vocab = list(common_vocab)\n",
        "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
        "\n",
        "    # Then for each model...\n",
        "    for m in [m1, m2]:\n",
        "        # Replace old syn0norm array with new one (with common vocab)\n",
        "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
        "        old_arr = m.wv.vectors\n",
        "        new_arr = np.array([old_arr[index] for index in indices])\n",
        "        m.wv.vectors = new_arr\n",
        "\n",
        "        # Replace old vocab dictionary with new one (with common vocab)\n",
        "        # and old index2word with new one\n",
        "        new_key_to_index = {}\n",
        "        new_index_to_key = []\n",
        "        for new_index, key in enumerate(common_vocab):\n",
        "            new_key_to_index[key] = new_index\n",
        "            new_index_to_key.append(key)\n",
        "        m.wv.key_to_index = new_key_to_index\n",
        "        m.wv.index_to_key = new_index_to_key\n",
        "\n",
        "    assert len(m1.wv.key_to_index) == len(m1.wv.key_to_index)\n",
        "\n",
        "    return m1, m2"
      ],
      "metadata": {
        "id": "BOJnazDOjZ_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smart_procrustes_align_gensim(vec_dir, base_path, other_path):\n",
        "    \"\"\"\n",
        "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
        "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
        "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
        "\n",
        "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
        "    Then do the alignment on the other_embed model.\n",
        "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
        "    Return other_embed.\n",
        "    \"\"\"\n",
        "\n",
        "    base_embed = Word2Vec.load(vec_dir + base_path + \".vec\")\n",
        "    other_embed = Word2Vec.load(vec_dir + other_path + \".vec\")\n",
        "\n",
        "    # make sure vocabulary and indices are aligned\n",
        "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed)\n",
        "\n",
        "    # get the (normalized) embedding matrices\n",
        "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
        "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
        "\n",
        "    # just a matrix dot product with numpy\n",
        "    m = other_vecs.T.dot(base_vecs)\n",
        "    # SVD method from numpy\n",
        "    u, _, v = np.linalg.svd(m)\n",
        "    # another matrix operation\n",
        "    ortho = u.dot(v)\n",
        "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
        "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)\n",
        "\n",
        "    in_base_embed.save(vec_dir + base_path + other_path[1] + \"_based_\" + base_path + \"_aligned.vec\")\n",
        "    other_embed.save(vec_dir + base_path + other_path[1] + \"_based_\" + other_path + \"_aligned.vec\")"
      ],
      "metadata": {
        "id": "OMOwks_hkXbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_embeddings(vec_dir):\n",
        "    \"\"\"\n",
        "    Pairwise alignment.\n",
        "    \"\"\"\n",
        "\n",
        "    pairs = [[\"c1\", \"c2\"], [\"c2\", \"c3\"], [\"c1\", \"c3\"]]\n",
        "\n",
        "    for pair in pairs:\n",
        "        smart_procrustes_align_gensim(vec_dir, pair[0], pair[1])"
      ],
      "metadata": {
        "id": "VpHv_c_WzXJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_context_free_representations(targets_path, model1_path, model2_path):\n",
        "    \"\"\" Compares aligned embeddings for all target words and makes a prediction. \"\"\"\n",
        "\n",
        "    with open(targets_path, 'r', encoding='utf-8') as f:\n",
        "        targets = [word.strip() for word in f.readlines()]\n",
        "\n",
        "    c1_dict = load_vector_dict(model1_path, targets)\n",
        "    c2_dict = load_vector_dict(model2_path, targets)\n",
        "\n",
        "    dists = [{\"word\": target, \"change\": cosine(c1_dict[target], c2_dict[target])} for target in targets if target in c1_dict and target in c2_dict]\n",
        "\n",
        "    return dists"
      ],
      "metadata": {
        "id": "MidfUB0-9V0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_all_representations(targets_path, vec_dir):\n",
        "    \"\"\"\n",
        "    Pairwise comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    pairs = [[\"c1\", \"c2\"], [\"c2\", \"c3\"], [\"c1\", \"c3\"]]\n",
        "\n",
        "    with open(targets_path, 'r', encoding='utf-8') as f:\n",
        "        targets = [word.strip() for word in f.readlines()]\n",
        "\n",
        "    dists = {target: [] for target in targets}\n",
        "    for pair in pairs:\n",
        "        model1_path = vec_dir + pair[0] + pair[1][1] + \"_based_\" + pair[0] + \"_aligned.vec\"\n",
        "        model2_path = vec_dir + pair[0] + pair[1][1] + \"_based_\" + pair[1] + \"_aligned.vec\"\n",
        "        new_dists = compare_context_free_representations(targets_path, model1_path, model2_path)\n",
        "\n",
        "        for item in new_dists:\n",
        "            dists[item[\"word\"]].append(item[\"change\"])\n",
        "\n",
        "    results = [{\"word\": word, \"presov-sov\": dists[word][0], \"sov-postsov\": dists[word][1], \"presov-postsov\": dists[word][2]} for word in dists]\n",
        "    pd.DataFrame(results).to_csv(vec_dir + \"results.csv\", sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "WjIRqoxq2_YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запуск"
      ],
      "metadata": {
        "id": "tZKSj0xEJn5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset_dir\n",
        "!mkdir /content/experiment_dir"
      ],
      "metadata": {
        "id": "GaZmOJTWDZpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1266bb-d7d0-4792-c926-1f9f36b121b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/dataset_dir’: File exists\n",
            "mkdir: cannot create directory ‘/content/experiment_dir’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/dataset_dir/corpus19.txt https://raw.githubusercontent.com/Timofeidedov/NLP_Finalproj/main/corpus1.txt\n",
        "!wget -O /content/dataset_dir/corpus20.txt https://raw.githubusercontent.com/Timofeidedov/NLP_Finalproj/main/corpus2.txt\n",
        "!wget -O /content/dataset_dir/corpus21.txt https://raw.githubusercontent.com/Timofeidedov/NLP_Finalproj/main/corpus3.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6exULcHcGby",
        "outputId": "df55a7b0-dfed-4e0e-ff40-858d20bb6a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 19:05:47--  https://raw.githubusercontent.com/Timofeidedov/NLP_Finalproj/main/corpus1.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1887276 (1.8M) [text/plain]\n",
            "Saving to: ‘/content/dataset_dir/corpus19.txt’\n",
            "\n",
            "/content/dataset_di 100%[===================>]   1.80M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-03-24 19:05:48 (50.1 MB/s) - ‘/content/dataset_dir/corpus19.txt’ saved [1887276/1887276]\n",
            "\n",
            "--2024-03-24 19:05:48--  https://raw.githubusercontent.com/Timofeidedov/NLP_Finalproj/main/corpus2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1723561 (1.6M) [text/plain]\n",
            "Saving to: ‘/content/dataset_dir/corpus20.txt’\n",
            "\n",
            "/content/dataset_di 100%[===================>]   1.64M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-03-24 19:05:48 (40.5 MB/s) - ‘/content/dataset_dir/corpus20.txt’ saved [1723561/1723561]\n",
            "\n",
            "--2024-03-24 19:05:49--  https://raw.githubusercontent.com/Timofeidedov/NLP_Finalproj/main/corpus3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899452 (1.8M) [text/plain]\n",
            "Saving to: ‘/content/dataset_dir/corpus21.txt’\n",
            "\n",
            "/content/dataset_di 100%[===================>]   1.81M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-03-24 19:05:49 (48.5 MB/s) - ‘/content/dataset_dir/corpus21.txt’ saved [1899452/1899452]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/dataset_dir/targets.tsv https://raw.githubusercontent.com/akutuzov/rushifteval_public/main/annotated_testset.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqHK-eGvbmrd",
        "outputId": "e2e6f197-e9dc-487d-ce13-06572309cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 19:05:49--  https://raw.githubusercontent.com/akutuzov/rushifteval_public/main/annotated_testset.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6509 (6.4K) [text/plain]\n",
            "Saving to: ‘/content/dataset_dir/targets.tsv’\n",
            "\n",
            "/content/dataset_di 100%[===================>]   6.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 19:05:49 (49.0 MB/s) - ‘/content/dataset_dir/targets.tsv’ saved [6509/6509]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets_fp = \"/content/dataset_dir/targets.tsv\"\n",
        "targets_df = pd.read_csv(targets_fp, names=[\"word\", \"rate1\", \"rate2\", \"rate3\"], delimiter='\\t')\n",
        "target_words = targets_df[\"word\"].to_list()\n",
        "\n",
        "target_words_fp = \"/content/dataset_dir/target_words.txt\"\n",
        "with open(target_words_fp, 'w', encoding=\"utf-8\") as f_target_words:\n",
        "    f_target_words.write('\\n'.join(target_words))"
      ],
      "metadata": {
        "id": "o0ozm67jcST1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1_path = \"/content/dataset_dir/corpus19.txt\"\n",
        "c2_path = \"/content/dataset_dir/corpus20.txt\"\n",
        "c3_path = \"/content/dataset_dir/corpus21.txt\"\n",
        "targets_path = \"/content/dataset_dir/target_words.txt\"\n",
        "experiment_dir = \"/content/experiment_dir/\"\n",
        "vec_dir = experiment_dir + \"word_representations/\""
      ],
      "metadata": {
        "id": "YBwVXRZCoOIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_texts(c1_path, c2_path, c3_path, targets_path, experiment_dir)"
      ],
      "metadata": {
        "id": "xzgl5lW4D4nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_word2vec(experiment_dir)"
      ],
      "metadata": {
        "id": "rHYX1kBQsbf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "align_embeddings(vec_dir)"
      ],
      "metadata": {
        "id": "e9qHWb4l0Njt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_all_representations(targets_path, vec_dir)"
      ],
      "metadata": {
        "id": "JfVXHh1z3Z0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
